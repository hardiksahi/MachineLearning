{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b648f8a-550c-4e82-9858-435f898b7c84",
   "metadata": {},
   "source": [
    "## Experiment with using Transformer LM to do sentence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcf0035-5c05-4279-9027-408ab8b8e344",
   "metadata": {},
   "source": [
    "1. Finetune a classifier head on top of pretrained BERT\n",
    "2. Take embeddings from pretrained BERT and train a classifier on top of it. This is not finetuning of BERT since BERT is used only for getting embeddings\n",
    "3. Finetune GPT based LM to classify sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b64061-83ae-4436-b1e0-fd22aed1d440",
   "metadata": {},
   "source": [
    "- Use pretrained DistilBERT model from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "322555d5-e64f-4774-89ed-c6732bdf7031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>:root { --jp-notebook-max-width: 100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>:root { --jp-notebook-max-width: 100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8e9edae-8ee0-4bbb-9fe1-b51d207bb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522b0ebe-5a60-49e5-b9f7-425430e49f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d04cba-d361-43bc-9ffa-9a338d499c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbbcbb7-5562-4ae5-82c4-5ee102dd17ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertModel, DistilBertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33d1a4ed-a732-47f5-b53f-f61661f8802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "379f8cc2-9734-4ca8-93fc-7de480860016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c30cf55-2ef0-4261-8eb0-893ae8d11af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38091be-002d-4fea-a6a7-19784cc05d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae9d9da-3a74-4c2c-a365-06394f68e87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db6f748e-bab9-4166-a66f-ea17cf2fd858",
   "metadata": {},
   "source": [
    "## 1. Finetune a classifier head on top of pretrained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d93b924f-331c-4b68-9594-3cbda7196873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None).rename(columns={0: \"text\", 1:\"label\"})\n",
    "# dev_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/dev.tsv', delimiter='\\t', header=None).rename(columns={0: \"text\", 1:\"label\"})\n",
    "# test_df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/test.tsv', delimiter='\\t', header=None).rename(columns={0: \"text\", 1:\"label\"})\n",
    "\n",
    "# dev_df.shape\n",
    "\n",
    "# base_url = \"https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/\"\n",
    "# data = load_dataset(\"csv\", data_files={\"train\": f\"{base_url}train.tsv\", \"validate\": f\"{base_url}dev.tsv\", \"test\": f\"{base_url}test.tsv\"}, delimiter=\"\\t\")\n",
    "\n",
    "# train_df[\"data_type\"] = \"train\"\n",
    "# dev_df[\"data_type\"] = \"dev\"\n",
    "# test_df[\"data_type\"] = \"test\"\n",
    "\n",
    "# df = pd.concat([train_df, dev_df, test_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "# print(f\"Shape of df: {df.shape}\")\n",
    "\n",
    "# df.head()\n",
    "\n",
    "# ## Quite balanced\n",
    "# df[df.data_type =='train'].label.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2afebe9-60f6-4ac3-8c3d-dd210634db1d",
   "metadata": {},
   "source": [
    "## Load dataset at https://huggingface.co/datasets/stanfordnlp/sst2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a9e45b-7338-4ed1-a957-0a37f9330d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('stanfordnlp/sst2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b73cf-050f-4432-8598-d5a2867bfdd0",
   "metadata": {},
   "source": [
    "DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. It was pretrainined with the following objectives:\n",
    "it was pretrained with three objectives:\n",
    "\n",
    "1. Distillation loss: the model was trained to return the same probabilities as the BERT base model.\n",
    "2. Masked language modeling (MLM): this is part of the original training loss of the BERT base model. When taking a sentence, the model randomly masks 15% of the words in the input then run the entire masked sentence through the model and has to predict the masked words. This is different from traditional recurrent neural networks (RNNs) that usually see the words one after the other, or from autoregressive models like GPT which internally mask the future tokens. It allows the model to learn a bidirectional representation of the sentence.\n",
    "3. Cosine embedding loss: the model was also trained to generate hidden states as close as possible as the BERT base model.\n",
    "\n",
    "https://huggingface.co/distilbert/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcd90c-e795-439d-8925-55bd31c9a995",
   "metadata": {},
   "source": [
    "## Step1: Get tokenizer for specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdb10f28-f325-47d6-9d62-87e75f0d61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hardiksahi/miniconda3/envs/personal_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Based on the name of the model(distilbert), AutoTokenizer automatically instantiates one of the tokenizer classes of the library from a pretrained model vocabulary.\n",
    "## https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer\n",
    "## WordPiece based tokizer\n",
    "## Returns DistilBertTokenizer or DistilBertTokenizerFast based on use_fast=True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "873b826e-e026-4a53-9abb-6325a45a01e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer model_max_length: 1000000000000000019884624838656\n",
      "tokenizer truncation_side: right\n",
      "tokenizer padding_side: right\n",
      "tokenizer model_input_names: ['input_ids', 'attention_mask']\n",
      "tokenizer bos_token: None\n",
      "tokenizer eos_token: None\n",
      "tokenizer unk_token: [UNK]\n",
      "tokenizer sep_token: [SEP]\n",
      "tokenizer pad_token: [PAD]\n",
      "tokenizer cls_token: [CLS]\n",
      "tokenizer mask_token: [MASK]\n"
     ]
    }
   ],
   "source": [
    "print(f\"tokenizer model_max_length: {tokenizer.model_max_length}\") ## A very large values => unreliable\n",
    "print(f\"tokenizer truncation_side: {tokenizer.truncation_side}\")\n",
    "print(f\"tokenizer padding_side: {tokenizer.padding_side}\") \n",
    "print(f\"tokenizer model_input_names: {tokenizer.model_input_names}\") \n",
    "print(f\"tokenizer bos_token: {tokenizer.bos_token}\") \n",
    "print(f\"tokenizer eos_token: {tokenizer.eos_token}\") \n",
    "print(f\"tokenizer unk_token: {tokenizer.unk_token}\") \n",
    "print(f\"tokenizer sep_token: {tokenizer.sep_token}\") \n",
    "print(f\"tokenizer pad_token: {tokenizer.pad_token}\") \n",
    "print(f\"tokenizer cls_token: {tokenizer.cls_token}\") \n",
    "print(f\"tokenizer mask_token: {tokenizer.mask_token}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3dafb96-b0a3-4759-8a1c-dd672a9ac83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT config: DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"transformers_version\": \"4.40.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Check configuration of pretrained DistilBERT model\n",
    "configuration = DistilBertConfig()\n",
    "print(f\"DistilBERT config: {configuration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad5d3dac-af8f-4f4f-8568-393dd98f273a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(df, text_column=\"text\"):\n",
    "    ## truncation=True ensures that sequences to be no longer than DistilBERT’s maximum input length\n",
    "    ## https://huggingface.co/docs/transformers/v4.40.1/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.__call__\n",
    "    return tokenizer(df[text_column], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c7a7d-166d-41ec-933d-1e440ac83b94",
   "metadata": {},
   "source": [
    "## tokenizer returns input_ids (token id) and attention_mask to be input to model\n",
    " https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1a25897-39ef-45b4-ac95-12adb1f160da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1037, 18385, 1010, 6057, 1998, 2633, 18276, 2128, 16603, 1997, 5053, 1998, 1996, 6841, 1998, 5687, 5469, 3152, 102], [101, 2026, 2171, 2003, 2524, 5480, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films', 'my name is hardik'], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbcd294-7d7e-4fe2-8187-88a50db94698",
   "metadata": {},
   "source": [
    "## encode returns input_ids\n",
    "https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ccb39ed-6afd-4eb8-9754-1a895d840e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoding = tokenizer.encode('A stirring , Funny and finally transporting re imagining of beauty and the beast and 1930s horror films amzertfys', truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2819c-9e3a-451f-bca6-107ec1ab976b",
   "metadata": {},
   "source": [
    "## decode converts token/ input_ids to tokens and returns sentences\n",
    "https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e0a747b-2bcd-4211-9656-fe3dc0ebda5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] a stirring, funny and finally transporting re imagining of beauty and the beast and 1930s horror films amzertfys [SEP]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sample_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a228b8eb-c77f-45ec-a531-6de2a4bbdf09",
   "metadata": {},
   "source": [
    "## See the tokenization (Wordpiece result) using convert_ids_to_tokens\n",
    "https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6051a84d-cede-4f11-9133-319ef6080059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'a',\n",
       " 'stirring',\n",
       " ',',\n",
       " 'funny',\n",
       " 'and',\n",
       " 'finally',\n",
       " 'transporting',\n",
       " 're',\n",
       " 'imagining',\n",
       " 'of',\n",
       " 'beauty',\n",
       " 'and',\n",
       " 'the',\n",
       " 'beast',\n",
       " 'and',\n",
       " '1930s',\n",
       " 'horror',\n",
       " 'films',\n",
       " 'am',\n",
       " '##zer',\n",
       " '##tf',\n",
       " '##ys',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(sample_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cf789-d81e-4c0f-9925-36a08ae9eca4",
   "metadata": {},
   "source": [
    "## Step2: Tokenize the entries in text column to get input_ids(token_ids) and attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0832cc6-7533-427f-8d05-b56bb78a3df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec431aab2374d23a3fb0216e1aff2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cc57240b59459bb4c8d133aac0b6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6067e81e874a476c8a5469b0b3fad8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenized_dict_list = preprocess_function(df, text_column=\"text\")\n",
    "tokenized_df = df.map(partial(preprocess_function, text_column=\"sentence\"), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e0fa5a6-7fa5-42fd-ad3a-4ee8cd38b27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'sentence', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdc99c-1447-4f1c-89d4-72ab590cf332",
   "metadata": {},
   "source": [
    "## Step3: Padd shorted sequences to ensure all are of length 512 tokens (In step 2 we truncated long sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b3dd5-16b3-4e04-a641-d47c971c0e71",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/data_collator#transformers.DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d6a06d-852a-44ee-8f17-0808322a8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca0f05-96c3-42b2-b21a-86473081bd8c",
   "metadata": {},
   "source": [
    "## Step 4: Get eveluation metric (scikit learn or evaluate library)\n",
    "https://huggingface.co/docs/evaluate/package_reference/loading_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9dd5de39-3889-4eb5-9dd9-9cb17423f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "##evaluate.list_evaluation_modules(module_type=\"metric\", include_community=True, with_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12590c2a-9096-4c84-9f66-a3dd176152de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nbansal/semf1']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[metric for metric in evaluate.list_evaluation_modules(module_type=\"metric\", include_community=True) if 'f1' in metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb0e3c8f-ae58-4688-b0a1-d56c1e1a5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c90ff801-af18-4a71-8257-15e00acb4f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy_value = accuracy(predictions=predictions, references=labels)\n",
    "    precision_value = precision(predictions=predictions, references=labels)\n",
    "    recall_value = recall(predictions=predictions, references=label)\n",
    "    f1_value = f1(predictions=predictions, references=label)\n",
    "    return accuracy_value, precision_value, recall_value, f1_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68edd58-6fc4-43de-a270-b561e2cb2040",
   "metadata": {},
   "source": [
    "## Step 5: Get id2label and label2id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6920e15-a790-49ec-97c2-b7249e7f33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:\"negative\", 1:\"positive\"}\n",
    "label2id = {\"negative\":0, \"positive\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba975b6f-36b4-4a57-801b-c5d8949c6266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21b2da3f-c130-45ad-8f6c-93de3678ffc6",
   "metadata": {},
   "source": [
    "## Step 6: Train model\n",
    "1. Use Trainer API by Hugging face which abstracts the training loop\n",
    "2. Manually write training loop in native Pytorch/ Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5e2d6-a27d-47af-8641-248ed1ea4000",
   "metadata": {},
   "source": [
    "### Step 6.1 Use Trainer API\n",
    "https://huggingface.co/docs/transformers/en/training#train-with-pytorch-trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7678aaf2-1faa-4dfe-bd84-fbdd1c013fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## https://huggingface.co/docs/transformers/v4.40.1/en/model_doc/auto#transformers.AutoModelForSequenceClassification\n",
    "## model with be instantiated with a classification head (Linear+Softmax)\n",
    "## https://huggingface.co/docs/transformers/v4.40.1/en/main_classes/configuration#transformers.PretrainedConfig\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd396fb4-bfe6-45c3-98bc-e4381f35b8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model with classification head architecture: DistilBertForSequenceClassification(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pretrained model with classification head architecture: {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f25fb-c03b-4bcf-af6c-692bfd8148cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.TrainingArguments\n",
    "training_arguments = TrainingArguments(output_dir=\"./results\", learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=5, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad812a-8342-4273-a9ce-d46bee39d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_arguments, train_dataset=tokenized_df[\"train\"], eval_dataset=tokenized_df[\"val\"], tokenizer=tokenizer, data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30467e6-715b-4780-adae-447af5e20a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b0adbf-e77c-4d52-a643-a8cef388e71a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal_env",
   "language": "python",
   "name": "personal_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
